{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KI Projekt Verkehrsszenario _ Verhaltensprädiktion\n",
    "\n",
    "### Importieren der benötigten Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importieren des Datensatzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"SIM_002.csv\", delimiter=\";\")\n",
    "df_input = pd.read_csv(\"Input.csv\", delimiter=\";\")\n",
    "df_all.dtypes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konvertieren der Objekt-Zahlenwerte zu Float Zahlenwerten\n",
    "Nachdem auffällt, dass v, v_left und v_front als object eingelesen werden, müssen diese Spalten hier noch konvertiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.replace(',','.', regex=True)\n",
    "df_all['v'] = pd.to_numeric(df_all['v'])\n",
    "df_all['v_left'] = pd.to_numeric(df_all['v_left'])\n",
    "df_all['v_front'] = pd.to_numeric(df_all['v_front'])\n",
    "\n",
    "df_input = df_input.replace(',','.', regex=True)\n",
    "df_input['v'] = pd.to_numeric(df_input['v'])\n",
    "df_input['v_left'] = pd.to_numeric(df_input['v_left'])\n",
    "df_input['v_front'] = pd.to_numeric(df_input['v_front'])\n",
    "\n",
    "\n",
    "df_all"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Umrechnen der Geschwindigkeiten von m/s auf Km/h\n",
    "Dies wird gemacht, um einheitliche Werte zu haben. Maximalgeschwindigkeit und die Geschwindigkeiten der Autos unterscheiden sich in der Einheit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['v'] = df_all['v'] * 3.6\n",
    "df_all['v_left'] = df_all['v_left'] * 3.6\n",
    "df_all['v_front'] = df_all['v_front'] * 3.6\n",
    "\n",
    "df_input['v'] = df_input['v'] * 3.6\n",
    "df_input['v_left'] = df_input['v_left'] * 3.6\n",
    "df_input['v_front'] = df_input['v_front'] * 3.6\n",
    "\n",
    "df_all"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bereinigen des Datensets wo notwendig\n",
    "\n",
    "Es ist keinerlei Datensatz aufgefallen, welcher entfernt werden muss, da er ungültige Werte (bspw. leere Felder oä) enthält."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding der nicht-nummerischen Werte\n",
    "Die folgenden features sind vom typ object und benötigen deshalb ein label encoding, damit später besser mit ihnen gearbeitet werden kann.\n",
    "Das label encoding wird mit der Bibliothek scikit learn durchgeführt.\n",
    "#### type-left:\n",
    "* car -> 0\n",
    "* motorcycle -> 1\n",
    "* sportscar -> 2\n",
    "* truck -> 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df_all['type_left'].value_counts())\n",
    "\n",
    "# Do the label encoding with sklearn\n",
    "type_left_encoder = preprocessing.LabelEncoder().fit(df_all[\"type_left\"])\n",
    "print(dict(zip(type_left_encoder.classes_, type_left_encoder.transform(type_left_encoder.classes_))))\n",
    "df_all[\"type_left\"] = type_left_encoder.transform(df_all[\"type_left\"])\n",
    "df_input[\"type_left\"] = type_left_encoder.transform(df_input[\"type_left\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### type-front:\n",
    "* car -> 0\n",
    "* motorcycle -> 1\n",
    "* sportscar -> 2\n",
    "* truck -> 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all['type_front'].value_counts())\n",
    "\n",
    "# Do the label encoding with sklearn\n",
    "type_front_encoder = preprocessing.LabelEncoder().fit(df_all[\"type_front\"])\n",
    "print(dict(zip(type_front_encoder.classes_, type_front_encoder.transform(type_front_encoder.classes_))))\n",
    "df_all[\"type_front\"] = type_front_encoder.transform(df_all[\"type_front\"])\n",
    "df_input[\"type_front\"] = type_front_encoder.transform(df_input[\"type_front\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### slope_street:\n",
    "* ascending -> 0\n",
    "* decending -> 1\n",
    "* flat -> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all['slope_street'].value_counts())\n",
    "\n",
    "# Do the label encoding with sklearn\n",
    "slope_street_encoder = preprocessing.LabelEncoder().fit(df_all[\"slope_street\"])\n",
    "print(dict(zip(slope_street_encoder.classes_, slope_street_encoder.transform(slope_street_encoder.classes_))))\n",
    "df_all[\"slope_street\"] = slope_street_encoder.transform(df_all[\"slope_street\"])\n",
    "df_input[\"slope_street\"] = slope_street_encoder.transform(df_input[\"slope_street\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### street_type:\n",
    "* autobahn -> 0\n",
    "* country_road (separated) -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all['street_type'].value_counts())\n",
    "\n",
    "# Do the label encoding with sklearn\n",
    "street_type_encoder = preprocessing.LabelEncoder().fit(df_all[\"street_type\"])\n",
    "print(dict(zip(street_type_encoder.classes_, street_type_encoder.transform(street_type_encoder.classes_))))\n",
    "df_all[\"street_type\"] = street_type_encoder.transform(df_all[\"street_type\"])\n",
    "df_input[\"street_type\"] = street_type_encoder.transform(df_input[\"street_type\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### time:\n",
    "* dawn -> 0\n",
    "* day -> 1\n",
    "* dusk -> 2\n",
    "* night -> 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all['time'].value_counts())\n",
    "\n",
    "# Do the label encoding with sklearn\n",
    "time_encoder = preprocessing.LabelEncoder().fit(df_all[\"time\"])\n",
    "print(dict(zip(time_encoder.classes_, time_encoder.transform(time_encoder.classes_))))\n",
    "df_all[\"time\"] = time_encoder.transform(df_all[\"time\"])\n",
    "df_input[\"time\"] = time_encoder.transform(df_input[\"time\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### weather:\n",
    "* dry -> 0\n",
    "* fog -> 1\n",
    "* rain -> 2\n",
    "* snow_ice -> 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all['weather'].value_counts())\n",
    "\n",
    "# Do the label encoding with sklearn\n",
    "weather_encoder = preprocessing.LabelEncoder().fit(df_all[\"weather\"])\n",
    "print(dict(zip(weather_encoder.classes_, weather_encoder.transform(weather_encoder.classes_))))\n",
    "df_all[\"weather\"] = weather_encoder.transform(df_all[\"weather\"])\n",
    "df_input[\"weather\"] = weather_encoder.transform(df_input[\"weather\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### type_vehicle:\n",
    "* car -> 0\n",
    "* motorcycle -> 1\n",
    "* sportscar -> 2\n",
    "* truck -> 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all['type_vehicle'].value_counts())\n",
    "\n",
    "# Do the label encoding with sklearn\n",
    "type_vehicle_encoder = preprocessing.LabelEncoder().fit(df_all[\"type_vehicle\"])\n",
    "print(dict(zip(type_vehicle_encoder.classes_, type_vehicle_encoder.transform(type_vehicle_encoder.classes_))))\n",
    "df_all[\"type_vehicle\"] = type_vehicle_encoder.transform(df_all[\"type_vehicle\"])\n",
    "df_input[\"type_vehicle\"] = type_vehicle_encoder.transform(df_input[\"type_vehicle\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### action:\n",
    "One Hot Encoding für alle action Werte um sie einzeln betrachten zu können für die Korrelationen. Dies ist eine Methode, um mit Attributen zu arbeiten, welche keine Nominalskala haben, die Korrelationen trotzdem interessant ist. Bei anderen Attributen wurde darauf verzichtet, da die Korrelation hier weniger interessant ist. Außerdem wird das encoding nur in eine Kopie des eigentlichen Datenframes geschrieben, um es später nicht für die Vorhersage zu nutzen, sondern nur für die Korrelationsheatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_all.copy()\n",
    "actions = ['accelerated_lane_change', 'continue', 'decelerate', 'lane_change']\n",
    "for x in actions:\n",
    "    df_analysis[\"{}\".format(x)] = np.where(df_analysis.action == x, 1, 0)\n",
    "    \n",
    "df_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "sns.heatmap(data=df_analysis.corr(numeric_only=True), vmin=-1, vmax=1, annot=True, cmap=\"magma\").set_title('Korrelation Heatmap aller Attribute', fontdict={'fontsize': 14}, pad=12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation der Korrelationsheatmap\n",
    "Da die Geschwindigkeit ein zentraler Faktor in der Überholungs-Überlegung ist, wird diese zuerst betrachtet. Sie korreliert in der Heatmap sehr stark mit v_left, ``v_front`` und ``d_front``. Außerdem ist auffällig, dass eine negative Korrelation mit ``type_front`` und ``type_vehicle`` besteht. Diese kann aber wegen nicht vorhandener Rangordnung in den Attributen allerdings nicht in Betracht gezogen werden. \n",
    "\n",
    "type_front korreliert zwar mit `v`, `v_left`, `v_front` und `d_front` negativ, allerdings ist auch hier keine Nominalskala vorhanden.\n",
    "\n",
    "Der wichtigste Faktor für das Training ist ``action``. Dieser Wert teilt sich hier in vier verschiedene Attribute auf. Es ist eine leichte Korrelation zwischen der ``action`` 'continue' und ``v_front`` zu erkennen. Vermutlich wird also eher 'continue' gewählt, wenn der Abstand zum vorrausfahrenden Auto größer ist. \n",
    "``v_front`` korreliert außerdem auch leicht negativ mit den anderen ``action``s, dabei am stärksten mit ``lane_change``. Also je kleiner der Abstand zum vorrausfahrenden Auto ist, desto eher wird abgebremst, Spur gewechselt oder beschleunigt die Spur gewechselt.\n",
    "\n",
    "Außerdem ist zwischen ``accelerated_lane_change`` und ``d_left`` eine leichte Korrelation zu erkennen. Je weiter das Auto auf der linken Spur also entfernt ist, desto größer ist die Wahrscheinlichkeit für einen beschleunigten Spurwechsel. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geschwindigkeit Differenz Berechnung zur action-Bewertung\n",
    "Dazu werden zum einem die Differnz der Geschwindigkeiten von 'v' und 'v_front' berechnet, um einschätzen zu können, ob die Geschwindigkeit beibehalten werden soll, ob verzögert oder überholt soll. \n",
    "\n",
    "Außerdem werden 'v' und 'v_left' verglichen, um bewerten zu können, ob und wie (mit oder ohne Beschleunigung) eine Überholung stattfinden soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(df_all) +1):\n",
    "    df_all['delta_vFront'] = df_all['v'] - df_all['v_front']\n",
    "    df_all['delta_vLeft'] = df_all['v'] - df_all['v_left']\n",
    "    df_all['delta_speed_limit'] = df_all['speed_limit(km/h)'] - df_all['v']\n",
    "    df_input['delta_vFront'] = df_input['v'] - df_input['v_front']\n",
    "    df_input['delta_vLeft'] = df_input['v'] - df_input['v_left']\n",
    "    df_input['delta_speed_limit'] = df_input['speed_limit(km/h)'] - df_input['v']\n",
    "\n",
    "df_all"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainings- und Testdaten aufteilen\n",
    "\n",
    "Die Daten werden hier aufgeteilt. Um später eine Cross-Validierung möglich zu machen, werden je nach Eingabe verschiedene Intervalle gewählt. Diese sind entweder mit den Testdaten bei 0 - 100, 100 - 200 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interval got to be between 0 and 4\n",
    "def get_test_and_train_data(interval):\n",
    "    lower_border = interval * 100\n",
    "    upper_border = lower_border + 100\n",
    "    if lower_border != 0:\n",
    "        train_data = df_all.loc[:lower_border]\n",
    "        train_data = train_data.append(df_all.loc[(upper_border+1):])\n",
    "        test_data = df_all.loc[(lower_border + 1):upper_border]\n",
    "    else:\n",
    "        train_data = df_all.loc[(upper_border):]\n",
    "        test_data = df_all.loc[(lower_border):upper_border-1]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prüfung auf Gleichheit \n",
    "In dieser Funktion werden die Testdaten auf Gleichheit überprüft. Das bedeutet, dass die Eingabe mit allen Trainingsdaten verglichen wird und auf eine Überseinstimmung untersucht wird. Sobald Trainingsdaten mit den exakt gleichen Werten gefunden wurden, wird die gleiche 'action' für die Testdaten übernommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_same(input, train_data):\n",
    "    for index, row in train_data.iterrows():\n",
    "        if input['v'] == row['v'] and input['v_left'] == row['v_left'] and input['v_front'] == row['v_front'] and input['d_left'] == row['d_left'] and input['d_front'] == row['d_front'] and input['type_left'] == row['type_left'] and input['type_front'] == row['type_front'] and input['radius_curve(m)'] == row['radius_curve(m)'] and input['slope_street'] == row['slope_street'] and input['street_type'] == row['street_type'] and input['time'] == row['time'] and input['weather'] == row['weather'] and input['type_vehicle'] == row['type_vehicle'] and input['speed_limit(km/h)'] == row['speed_limit(km/h)'] and input['delta_vFront'] == row['delta_vFront'] and input['delta_vLeft'] == row['delta_vLeft']:\n",
    "            return True, row\n",
    "    return False, 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prüfung auf Ähnlichkeit\n",
    "\n",
    "Nachdem auf Gleichheit geprüft wurde und der Fall auftritt, dass keine genau gleichen Trainingsdaten gefunden wurden, wird auf Ähnlichkeit geprüft. \n",
    "Das heißt, dass alle Attribute verglichen werden und nach dem ähnlichsten Fall in den Trainingsdaten gesucht wird. \n",
    "\n",
    "Um am Schluss eine Formel für das Ähnlichkeitsmaß aufstellen zu können, müssen die verschiedenen Attribute alle einen Wert bekommen, der in dem Bereich von 0 und 1 liegt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity of speed with a value between 0 and 1 \n",
    "# default and to_compare are the input and training data\n",
    "# both, default and to_compare, contains positive values \n",
    "def similarity_v(default, to_compare):\n",
    "    return 1 - ((abs(default - to_compare))/(abs(default) + abs(to_compare)))\n",
    "\n",
    "def similarity_v_left(default, to_compare):\n",
    "    return similarity_v(default, to_compare)\n",
    "\n",
    "def similarity_v_front(default, to_compare):\n",
    "    return similarity_v(default, to_compare)\n",
    "\n",
    "# d_left contains negative values as well and therefore a different actions are necessary\n",
    "def similarity_d_left(default, to_compare):\n",
    "    if default == 0 and to_compare == 0: # both 0 \n",
    "        return 1\n",
    "    elif default >= 0 and to_compare >= 0: # both positive \n",
    "        return similarity_v(default, to_compare)\n",
    "    elif default < 0 and to_compare < 0: # both negative \n",
    "        return similarity_v(default, to_compare)\n",
    "    # if one number is negative, take its abs value and keep the distance between the numbers\n",
    "    elif default < 0 and to_compare >= 0:\n",
    "        default = abs(default)\n",
    "        to_compare += 2 * default\n",
    "        return similarity_v(default, to_compare)\n",
    "    elif default >= 0 and to_compare < 0:\n",
    "        to_compare = abs(to_compare)\n",
    "        default += 2 * to_compare\n",
    "        return similarity_v(default, to_compare)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# similarity of positive numeric values: \n",
    "def similarity_d_front(default, to_compare):\n",
    "    return similarity_v(default, to_compare)\n",
    "\n",
    "def similarity_radius_curve(default, to_compare):\n",
    "    return similarity_v(default, to_compare)\n",
    "\n",
    "def similarity_speed_limit(default, to_compare):\n",
    "    return similarity_v(default, to_compare)\n",
    "\n",
    "def similarity_delta_vFront(default, to_compare):\n",
    "    return similarity_v(default, to_compare)\n",
    "\n",
    "# similarity of mixed numeric values -> therefore similarity_d_left is necessary\n",
    "def similarity_delta_vLeft(default, to_compare):\n",
    "    return similarity_d_left(default, to_compare)\n",
    "\n",
    "def similarity_delta_speed_limit(default, to_compare):\n",
    "    return similarity_d_left(default, to_compare)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distanzberechnung der type_left, type_front und type_vehicle Fahrzeuge\n",
    "0: motorcycle and truck<br>\n",
    "0.1: sportscar and truck<br>\n",
    "0.3: car and motorcycle<br>\n",
    "0.5: sportscar and motorcycle<br>\n",
    "0.6: car and truck<br>\n",
    "0.8: car and sportscar<br>\n",
    "1: same<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = car, 1 = motorcycle, 2 = sportscar, 3 = truck\n",
    "def similarity_type_left(default, to_compare):\n",
    "    if (default == 1 and to_compare == 3) or (default == 3 and to_compare == 1): # motorcycle and truck\n",
    "        return 0.0\n",
    "    if (default == 2 and to_compare == 3) or (default == 3 and to_compare == 2): # sportscar and truck\n",
    "        return 0.1\n",
    "    if (default == 0 and to_compare == 1) or (default == 1 and to_compare == 0): # car and motorcycle\n",
    "        return 0.3\n",
    "    if (default == 2 and to_compare == 1) or (default == 1 and to_compare == 2): # sportscar and motorcycle\n",
    "        return 0.5\n",
    "    if (default == 0 and to_compare == 3) or (default == 3 and to_compare == 0): # car and truck\n",
    "        return 0.6\n",
    "    if (default == 0 and to_compare == 2) or (default == 2 and to_compare == 0): # car and sportscar\n",
    "        return 0.8    \n",
    "    if (default == to_compare): # same\n",
    "        return 1.0\n",
    "\n",
    "# same vehicles and encodings \n",
    "def similarity_type_front(default, to_compare):\n",
    "    return similarity_type_left(default, to_compare)\n",
    "\n",
    "# same vehicles and encodings \n",
    "def similarity_type_vehicle(default, to_compare):\n",
    "    return similarity_type_left(default, to_compare)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distanzberechnung der Routenbeschaffenheit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ascending = 0, descending = 1, flat = 2\n",
    "def similarity_slope_street(default, to_compare):\n",
    "    if (default == 1 and to_compare == 2) or (default == 2 and to_compare == 1): # descending and flat\n",
    "        return 0.5\n",
    "    elif (default == 0 and to_compare == 2) or (default == 2 and to_compare == 0): # ascending and flat\n",
    "        return 0.5\n",
    "    elif default == to_compare:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distanzberechnung der Straßenart "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to different types of streets \n",
    "# same -> 1 \n",
    "# different (autobahn and country_road) -> 0\n",
    "def similarity_street_type(default, to_compare):\n",
    "    if default == to_compare:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distanzberechnung für das Wetter\n",
    "0.0: dry and snow ice<br>\n",
    "0.1: fog and snow ice<br>\n",
    "0.1: rain and snow ice<br>\n",
    "0.5: dry and fog<br>\n",
    "0.6: fog and rain<br>\n",
    "0.8: dry and rain<br>\n",
    "1: same<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dry = 0, fog = 1, rain = 2, snow_ice = 3 \n",
    "def similarity_weather(default, to_compare):\n",
    "    if (default == 0 and to_compare == 3) or (default == 3 and to_compare == 0): # dry and snow-ice \n",
    "        return 0\n",
    "    if (default == 1 and to_compare == 3) or (default == 3 and to_compare == 1): # fog and snow-ice \n",
    "        return 0.1\n",
    "    if (default == 2 and to_compare == 3) or (default == 3 and to_compare == 2): # rain and snow-ice \n",
    "        return 0.1\n",
    "    if (default == 0 and to_compare == 1) or (default == 1 and to_compare == 0): # dry and fog \n",
    "        return 0.5\n",
    "    if (default == 2 and to_compare == 1) or (default == 1 and to_compare == 2): # fog and rain\n",
    "        return 0.6\n",
    "    if (default == 2 and to_compare == 0) or (default == 0 and to_compare == 2): # dry and rain\n",
    "        return 0.8\n",
    "    if (default == to_compare): # same\n",
    "        return 1.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distanzberechnung für die verschiedenen Tageszeiten\n",
    "0: day and night<br>\n",
    "0.4: dusk and night<br>\n",
    "0.5: dawn and night<br>\n",
    "0.5: day and dawn<br>\n",
    "0.6: day and dusk<br>\n",
    "0.8: dawn and dusk<br>\n",
    "1: same<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dawn = 0, day = 1, dusk = 2, night = 3\n",
    "def similarity_time(default, to_compare):\n",
    "    if (default == 1 and to_compare == 3) or (default == 3 and to_compare == 1): # day and night\n",
    "        return 0.0\n",
    "    if (default == 2 and to_compare == 3) or (default == 3 and to_compare == 2): # dusk and night\n",
    "        return 0.4\n",
    "    if (default == 0 and to_compare == 3) or (default == 3 and to_compare == 0): # dawn and night\n",
    "        return 0.5\n",
    "    if (default == 0 and to_compare == 1) or (default == 1 and to_compare == 0): # day and dawn\n",
    "        return 0.5\n",
    "    if (default == 1 and to_compare == 2) or (default == 2 and to_compare == 1): # day and dusk\n",
    "        return 0.6\n",
    "    if (default == 0 and to_compare == 2) or (default == 2 and to_compare == 0): # dawn and dusk\n",
    "        return 0.8    \n",
    "    if (default == to_compare): # same\n",
    "        return 1.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das ähnlichste Element finden\n",
    "Zuerst werden die Gewichtungen für jedes einzelne Attribut festgelegt. Anschließend wird die Ähnlichkeit (similarity) berechnet. Dies funktioniert über Gewichtung * Ähnlichkeit, beide Werte zwischen null und eins. Anschließend werden die gewichteten Ähnlichkeiten aller Attribute aufsummiert und mit dem bisher ähnlichsten Fall verglichen. Wenn neue Fall ein höheres Ähnlichkeitsmaß erreicht, wird er selbst zum bisher höchsten gefundenen. Am Ende der Schleife wird die in dem Moment ähnlichste Situation zurückgegeben.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to find the most similar case \n",
    "def check_for_similar(input, train_data):\n",
    "    # weights setting:\n",
    "    weight_v = 1\n",
    "    weight_v_left = 1\n",
    "    weight_v_front = 0.4\n",
    "    weight_d_left = 1\n",
    "    weight_d_front = 1\n",
    "    weight_type_left = 0.1\n",
    "    weight_type_front = 0.1\n",
    "    weight_radius_curve = 0.7\n",
    "    weight_slope_street = 0.1\n",
    "    weight_street_type = 0.1\n",
    "    weight_time = 0.1\n",
    "    weight_weather = 0.8\n",
    "    weight_type_vehicle = 0.1\n",
    "    weight_speed_limit = 1\n",
    "    weight_delta_vFront = 0.8\n",
    "    weight_delta_vLeft = 0.4\n",
    "    weight_delta_speed_limit = 0.2\n",
    "\n",
    "    max_sim = 0\n",
    "    max_row = 0\n",
    "\n",
    "    # comparing attributes from training_data and input \n",
    "    for i in range(len(train_data)):\n",
    "        row = train_data.iloc[i]\n",
    "        # similarity calculation \n",
    "        similarity = weight_v * similarity_v(input['v'], row['v'])\\\n",
    "        + weight_v_left * similarity_v_left(input['v_left'], row['v_left']) \\\n",
    "        + weight_v_front * similarity_v_front(input['v_front'], row['v_front']) \\\n",
    "        + weight_d_left * similarity_d_left(input['d_left'], row['d_left'])  \\\n",
    "        + weight_d_front * similarity_d_front(input['d_front'], row['d_front'])  \\\n",
    "        + weight_type_left * similarity_type_left(input['type_left'], row['type_left'])  \\\n",
    "        + weight_type_front * similarity_type_front(input['type_front'], row['type_front']) \\\n",
    "        + weight_radius_curve * similarity_radius_curve(input['radius_curve(m)'],row['radius_curve(m)']) \\\n",
    "        + weight_slope_street * similarity_slope_street(input['slope_street'], row['slope_street']) \\\n",
    "        + weight_street_type * similarity_street_type(input['street_type'], row['street_type']) \\\n",
    "        + weight_time * similarity_time(input['time'], row['time']) \\\n",
    "        + weight_weather * similarity_weather(input['weather'], row['weather']) \\\n",
    "        + weight_type_vehicle * similarity_type_vehicle(input['type_vehicle'], row['type_vehicle']) \\\n",
    "        + weight_speed_limit * similarity_speed_limit(input['speed_limit(km/h)'], row['speed_limit(km/h)']) \\\n",
    "        + weight_delta_vFront * similarity_delta_vFront(input['delta_vFront'], row['delta_vFront']) \\\n",
    "        + weight_delta_vLeft *  similarity_delta_vLeft(input['delta_vLeft'], row['delta_vLeft']) \\\n",
    "        + weight_delta_speed_limit * similarity_delta_speed_limit(input['delta_vFront'], row['delta_vFront'])\n",
    "        if similarity > max_sim: # if current similarity bigger than the previous one, use the current row \n",
    "            max_row = row\n",
    "            max_sim = similarity\n",
    "        \n",
    "     # use highest similarity and the corresponding action for return\n",
    "    return max_row"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorhersage der Action\n",
    "Es wird zuerst geschaut, ob ein gleicher Fall in den Daten gefunden wird. Falls dies der Fall ist, wird sich gleich verhalten wie in dem gefundenen Fall. Wenn kein identischer Fall vorhanden ist, wird mit ``check_for_similar`` nach dem ähnlichsten Fall gesucht. Es wird sich dann so verhalten, wie in dem gefundenen Fall. Für bessere Nachvollziehbarkeit kann die Flag ``print_output`` genutzt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_prediction(input, train_data, print_output = False):\n",
    "    # first check for equal data \n",
    "    success, prediction = check_for_same(input, train_data)\n",
    "    # if no matching data was found -> check for the most similar case \n",
    "    if not success:\n",
    "        prediction = check_for_similar(input, train_data)\n",
    "    # debug mode\n",
    "    if print_output:\n",
    "        print('Input: \\n', input)\n",
    "        print(\"\\n-Prediction-\\n\")\n",
    "        print('Prediction: \\n', prediction)\n",
    "    return prediction['action']\n",
    "\n",
    "# input prediction:\n",
    "print('\\n\\nPredicted action for input case: ', do_prediction(df_input.iloc[0], df_all, print_output=True))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Berechnung der Accuracy\n",
    "\n",
    "Dabei wird die Genauigkeit des Modells berechnet, indem alle fünf Durchläufe mit den unterschiedlichen Test- und Trainingsdaten aufsummiert werden und dann durch die Länge, also die Anzahl der Durchläufe, geteilt wird. \n",
    "\n",
    "Außerdem wird nach der Suche einer Eingabe, die Eingabe und die Lösung des ähnlichen Falls aus den Trainingsdaten weiter verwendet. Dafür werden die Daten dem Trainingsdatensatz hinzugefügt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def evaluate_accuracy(add_test_line = False):\n",
    "    accuracy_sum = []\n",
    "    for j in range(0,5):\n",
    "        train_data, test_data = get_test_and_train_data(j)\n",
    "        accuracy = 0\n",
    "        for i in range(len(test_data)):\n",
    "            row = test_data.iloc[i]\n",
    "            prediction = do_prediction(row, train_data)\n",
    "            if prediction == row['action']:\n",
    "                accuracy += 1\n",
    "            # Retain phase\n",
    "            if add_test_line:\n",
    "                train_data = train_data.append(row)\n",
    "        accuracy = accuracy / len(test_data)\n",
    "        print('accuracy ', str(j+1), ' is: ', accuracy, ' / ', len(test_data))\n",
    "        accuracy_sum.append(accuracy)\n",
    "    accuracy_average = np.sum(accuracy_sum)/len(accuracy_sum)\n",
    "    print('accuracy total: ', accuracy_average)\n",
    "evaluate_accuracy(False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce7d75e9658a8d85a66eb6c604707347ed682486db6d86ccea4376d642eb57ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
